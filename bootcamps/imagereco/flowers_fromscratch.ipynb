{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flowers_fromscratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5TZUiGHl62Si",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Flowers Image Classification with TensorFlow on Cloud ML Engine\n",
        "\n",
        "This notebook demonstrates how to do image classification from scratch on a flowers dataset using the Estimator API."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ETEAhV4j62Sl",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "#@markdown Enter  your GCP Project ID:\n",
        "PROJECT = \"\" #@param {type: \"string\"}\n",
        "#@markdown Enter  your GCP Storage Bucket ID:\n",
        "BUCKET = \"\" #@param {type: \"string\"}\n",
        "#@markdown OPTIONAL: Replace with your GCP Storage Bucket Region:\n",
        "REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "\n",
        "MODEL_TYPE='cnn'       # convolutional neural network\n",
        "\n",
        "# do not change these\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['MODEL_TYPE'] = MODEL_TYPE\n",
        "os.environ['TFVERSION'] = '1.10'  # Tensorflow version\n",
        "\n",
        "def start_tensorboard(logdir, url_file):\n",
        "  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(logdir))\n",
        "  get_ipython().system_raw('lt --port 6006 >> {} 2>&1 &'.format(url_file))\n",
        "  get_ipython().system('cat {}'.format(url_file))\n",
        "\n",
        "def stop_tensorboard(url_file):\n",
        "  get_ipython().system_raw(\"ps -Af  | grep -E 'tensorboard|lt --port' | awk '{print $2}' | xargs -I % kill -9 %\")\n",
        "  get_ipython().system_raw(\"rm {}\".format(url_file))\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"Authenticated\")\n",
        "except:\n",
        "  print(\"Failed to authenticate\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5AIk8mmI62So",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/osipov/training-data-analyst.git\n",
        "cp -r training-data-analyst/bootcamps/imagereco/flowersmodel .\n",
        "\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "dHI2vGJM62St",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Input functions to read JPEG images\n",
        "\n",
        "The key difference between this notebook and [the MNIST one](./mnist_models.ipynb) is in the input function.\n",
        "In the input function here, we are doing the following:\n",
        "* Reading JPEG images, rather than 2D integer arrays.\n",
        "* Reading in batches of batch_size images rather than slicing our in-memory structure to be batch_size images.\n",
        "* Resizing the images to the expected HEIGHT, WIDTH. Because this is a real-world dataset, the images are of different sizes. We need to preprocess the data to, at the very least, resize them to constant size."
      ]
    },
    {
      "metadata": {
        "id": "81MYBSzXdE0i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile flowersmodel/model.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# Licensed under the Apache License, Version 2.0 See footer for details.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "LIST_OF_LABELS = 'daisy,dandelion,roses,sunflowers,tulips'.split(',')\n",
        "HEIGHT = 299\n",
        "WIDTH = 299\n",
        "NUM_CHANNELS = 3\n",
        "NCLASSES = 5\n",
        "\n",
        "def make_input_fn(csv_of_filenames, batch_size, mode, augment=False):\n",
        "    def _input_fn(): \n",
        "        def decode_csv(csv_row):\n",
        "            filename, label = tf.decode_csv(\n",
        "                csv_row, record_defaults = [[''],['']])\n",
        "            image_bytes = tf.read_file(filename)\n",
        "            return image_bytes, label\n",
        "        \n",
        "        # Create tf.data.dataset from filename\n",
        "        dataset = tf.data.TextLineDataset(csv_of_filenames).map(decode_csv)     \n",
        "        \n",
        "        if augment: \n",
        "            dataset = dataset.map(read_and_preprocess_with_augment)\n",
        "        else:\n",
        "            dataset = dataset.map(read_and_preprocess)\n",
        "\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        " \n",
        "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _input_fn \n",
        "  \n",
        "def read_and_preprocess_with_augment(image_bytes, label=None):\n",
        "    return read_and_preprocess(image_bytes, label, augment=True)\n",
        "    \n",
        "def read_and_preprocess(image_bytes, label=None, augment=False):\n",
        "    # decode the image\n",
        "    # end up with pixel values that are in the -1, 1 range\n",
        "    image = tf.image.decode_jpeg(image_bytes, channels=NUM_CHANNELS)\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # 0-1\n",
        "    image = tf.expand_dims(image, 0) # resize_bilinear needs batches\n",
        "    \n",
        "    if augment:\n",
        "       image = tf.image.resize_bilinear(\n",
        "           image, [HEIGHT+10, WIDTH+10], align_corners=False)\n",
        "       image = tf.squeeze(image) #remove batch dimension\n",
        "       image = tf.random_crop(image, [HEIGHT, WIDTH, NUM_CHANNELS])\n",
        "       image = tf.image.random_flip_left_right(image)\n",
        "       image = tf.image.random_brightness(image, max_delta=63.0/255.0)\n",
        "       image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
        "    else:\n",
        "       image = tf.image.resize_bilinear(image, [HEIGHT, WIDTH], align_corners=False)\n",
        "       image = tf.squeeze(image) #remove batch dimension\n",
        "        \n",
        "    #pixel values are in range [0,1], convert to [-1,1]\n",
        "    image = tf.subtract(image, 0.5)\n",
        "    image = tf.multiply(image, 2.0)\n",
        "    return {'image':image}, label\n",
        "\n",
        "def linear_model(img, mode, hparams):\n",
        "  X = tf.reshape(img,[-1,HEIGHT*WIDTH*NUM_CHANNELS]) #flatten\n",
        "  ylogits = tf.layers.dense(X,NCLASSES,activation=None)\n",
        "  return ylogits, NCLASSES\n",
        "\n",
        "def dnn_model(img, mode, hparams):\n",
        "  X = tf.reshape(img, [-1, HEIGHT*WIDTH*NUM_CHANNELS]) #flatten\n",
        "  h1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n",
        "  h2 = tf.layers.dense(h1,100, activation=tf.nn.relu)\n",
        "  h3 = tf.layers.dense(h2, 30, activation=tf.nn.relu)\n",
        "  ylogits = tf.layers.dense(h3, NCLASSES, activation=None)\n",
        "  return ylogits, NCLASSES\n",
        "\n",
        "def dnn_dropout_model(img, mode, hparams):\n",
        "  dprob = hparams.get('dprob', 0.1)\n",
        "\n",
        "  X = tf.reshape(img, [-1, HEIGHT*WIDTH*NUM_CHANNELS]) #flatten\n",
        "  h1 = tf.layers.dense(X, 300, activation=tf.nn.relu)\n",
        "  h2 = tf.layers.dense(h1,100, activation=tf.nn.relu)\n",
        "  h3 = tf.layers.dense(h2, 30, activation=tf.nn.relu)\n",
        "  h3d = tf.layers.dropout(h3, rate=dprob, training=(\n",
        "      mode == tf.estimator.ModeKeys.TRAIN)) #only dropout when training\n",
        "  ylogits = tf.layers.dense(h3d, NCLASSES, activation=None)\n",
        "  return ylogits, NCLASSES\n",
        "\n",
        "def cnn_model(img, mode, hparams):\n",
        "  ksize1 = hparams.get('ksize1', 5)\n",
        "  ksize2 = hparams.get('ksize2', 5)\n",
        "  nfil1 = hparams.get('nfil1', 10)\n",
        "  nfil2 = hparams.get('nfil2', 20)\n",
        "  dprob = hparams.get('dprob', 0.25)\n",
        "  \n",
        "  c1 = tf.layers.conv2d(img, filters=nfil1,\n",
        "                          kernel_size=ksize1, strides=1, \n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "  p1 = tf.layers.max_pooling2d(c1,pool_size=2, strides=2) \n",
        "  c2 = tf.layers.conv2d(p1, filters=nfil2,\n",
        "                          kernel_size=ksize2, strides=1, \n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "  p2 = tf.layers.max_pooling2d(c2,pool_size=2, strides=2)\n",
        "  \n",
        "  outlen = p2.shape[1]*p2.shape[2]*p2.shape[3] \n",
        "  p2flat = tf.reshape(p2, [-1, outlen]) # flattened\n",
        "\n",
        "  #apply batch normalization\n",
        "  if hparams['batch_norm']:\n",
        "    h3 = tf.layers.dense(p2flat, 300, activation=None)\n",
        "    h3 = tf.layers.batch_normalization(\n",
        "        h3, training=(mode == tf.estimator.ModeKeys.TRAIN)) #only batchnorm when training\n",
        "    h3 = tf.nn.relu(h3)\n",
        "  else:  \n",
        "    h3 = tf.layers.dense(p2flat, 300, activation=tf.nn.relu)\n",
        "  \n",
        "  #apply dropout\n",
        "  h3d = tf.layers.dropout(h3, rate=dprob, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
        "\n",
        "  ylogits = tf.layers.dense(h3d, NCLASSES, activation=None)\n",
        "  \n",
        "  #apply batch normalization once more\n",
        "  if hparams['batch_norm']:\n",
        "     ylogits = tf.layers.batch_normalization(\n",
        "         ylogits, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
        "\n",
        "  return ylogits, NCLASSES\n",
        "\n",
        "def serving_input_fn():\n",
        "    # Note: only handles one image at a time \n",
        "    feature_placeholders = {'image_bytes': \n",
        "                            tf.placeholder(tf.string, shape=())}\n",
        "    image, _ = read_and_preprocess(\n",
        "        tf.squeeze(feature_placeholders['image_bytes']))\n",
        "    image['image'] = tf.expand_dims(image['image'],0)\n",
        "    return tf.estimator.export.ServingInputReceiver(image, feature_placeholders)\n",
        "\n",
        "def image_classifier(features, labels, mode, params):\n",
        "  model_functions = {\n",
        "      'linear':linear_model,\n",
        "      'dnn':dnn_model,\n",
        "      'dnn_dropout':dnn_dropout_model,\n",
        "      'cnn':cnn_model}\n",
        "  model_function = model_functions[params['model']] \n",
        "  ylogits, nclasses = model_function(features['image'], mode, params)\n",
        "\n",
        "  probabilities = tf.nn.softmax(ylogits)\n",
        "  class_int = tf.cast(tf.argmax(probabilities, 1), tf.uint8)\n",
        "  class_str = tf.gather(LIST_OF_LABELS, tf.cast(class_int, tf.int32))\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "    #convert string label to int\n",
        "    labels_table = tf.contrib.lookup.index_table_from_tensor(\n",
        "      tf.constant(LIST_OF_LABELS))\n",
        "    labels = labels_table.lookup(labels)\n",
        "    \n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=ylogits, labels=tf.one_hot(labels, nclasses)))\n",
        "    evalmetrics =  {'accuracy': tf.metrics.accuracy(class_int, labels)}\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      # this is needed for batch normalization, but has no effect otherwise\n",
        "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "      with tf.control_dependencies(update_ops):\n",
        "         train_op = tf.contrib.layers.optimize_loss(\n",
        "             loss, \n",
        "             tf.train.get_global_step(),\n",
        "             learning_rate=params['learning_rate'],\n",
        "             optimizer=\"Adam\")\n",
        "    else:\n",
        "      train_op = None\n",
        "  else:\n",
        "    loss = None\n",
        "    train_op = None\n",
        "    evalmetrics = None\n",
        " \n",
        "  return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions={\"probabilities\": probabilities, \n",
        "                     \"classid\": class_int, \"class\": class_str},\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        eval_metric_ops=evalmetrics,\n",
        "        export_outputs={'classes': tf.estimator.export.PredictOutput(\n",
        "            {\"probabilities\": probabilities, \"classid\": class_int, \n",
        "             \"class\": class_str})}\n",
        "    )\n",
        "\n",
        "def train_and_evaluate(output_dir, hparams):\n",
        "  EVAL_INTERVAL = 300 #every 5 minutes    \n",
        "  estimator = tf.estimator.Estimator(model_fn = image_classifier,\n",
        "                                     params = hparams,\n",
        "                                     config= tf.estimator.RunConfig(\n",
        "                                         save_checkpoints_secs = EVAL_INTERVAL),\n",
        "                                     model_dir = output_dir)\n",
        "  train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(\n",
        "                                        hparams['train_data_path'],\n",
        "                                        hparams['batch_size'],\n",
        "                                        mode = tf.estimator.ModeKeys.TRAIN,\n",
        "                                        augment = hparams['augment']),\n",
        "                                      max_steps = hparams['train_steps'])\n",
        "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(\n",
        "                                        hparams['eval_data_path'],\n",
        "                                        hparams['batch_size'],\n",
        "                                        mode = tf.estimator.ModeKeys.EVAL),\n",
        "                                    steps = None,\n",
        "                                    exporters = exporter,\n",
        "                                    start_delay_secs = EVAL_INTERVAL,\n",
        "                                    throttle_secs = EVAL_INTERVAL)\n",
        "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "\n",
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "fS5WTTXE62Sv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run as a Python module\n",
        "\n",
        "Let's first run it locally for a short while to test the code works. Note the --model parameter"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jrNR_fQI62Sw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf flowersmodel.tar.gz flowers_trained\n",
        "gcloud ml-engine local train \\\n",
        "   --module-name=flowersmodel.task \\\n",
        "   --package-path=${PWD}/flowersmodel \\\n",
        "   -- \\\n",
        "   --output_dir=${PWD}/flowers_trained \\\n",
        "   --train_steps=5 \\\n",
        "   --learning_rate=0.01 \\\n",
        "   --batch_size=2 \\\n",
        "   --model=$MODEL_TYPE \\\n",
        "   --augment \\\n",
        "   --train_data_path=gs://cloud-ml-data/img/flower_photos/train_set.csv \\\n",
        "   --eval_data_path=gs://cloud-ml-data/img/flower_photos/eval_set.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KtIJO4jx62S0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's do it on ML Engine. Note the --model parameter"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "RVgA0RhP62S2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/flowers/trained_${MODEL_TYPE}\n",
        "JOBNAME=flowers_${MODEL_TYPE}_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=flowersmodel.task \\\n",
        "   --package-path=${PWD}/flowersmodel \\\n",
        "   --job-dir=$OUTDIR \\\n",
        "   --staging-bucket=gs://$BUCKET \\\n",
        "   --scale-tier=BASIC_GPU \\\n",
        "   --runtime-version=$TFVERSION \\\n",
        "   -- \\\n",
        "   --output_dir=$OUTDIR \\\n",
        "   --train_steps=1000 \\\n",
        "   --learning_rate=0.01 \\\n",
        "   --batch_size=40 \\\n",
        "   --model=$MODEL_TYPE \\\n",
        "   --augment \\\n",
        "   --batch_norm \\\n",
        "   --train_data_path=gs://cloud-ml-data/img/flower_photos/train_set.csv \\\n",
        "   --eval_data_path=gs://cloud-ml-data/img/flower_photos/eval_set.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Jtatrnru62S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Monitoring training with TensorBoard\n",
        "\n",
        "Use this cell to launch tensorboard"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "oYldtrFL62S6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel\n",
        "start_tensorboard('gs://{}/flowers/trained_{}'.format(BUCKET, MODEL_TYPE), 'url')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IEzLPdEw62S9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%sx read -p 'Press Enter in the input box to stop TensorBoard '\n",
        "stop_tensorboard('url')\n",
        "print(\"Stopped\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "sQCGx9bP62TB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here are my results:\n",
        "\n",
        "Model | Accuracy | Time taken | Run time parameters\n",
        "--- | :---: | ---\n",
        "cnn with batch-norm | 0.582 | 47 min | 1000 steps, LR=0.01, Batch=40\n",
        "as above, plus augment | 0.615 | 3 hr | 5000 steps, LR=0.01, Batch=40"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "y22SvZwe62TC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deploying and predicting with model\n",
        "\n",
        "Deploy the model:"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "esbuoc2L62TE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "MODEL_NAME=\"flowers\"\n",
        "MODEL_VERSION=${MODEL_TYPE}\n",
        "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/flowers/trained_${MODEL_TYPE}/export/exporter | tail -1)\n",
        "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
        "#gcloud ml-engine versions delete --quiet ${MODEL_VERSION} --model ${MODEL_NAME}\n",
        "#gcloud ml-engine models delete ${MODEL_NAME}\n",
        "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
        "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qYyikCra62TH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To predict with the model, let's take one of the example images that is available on Google Cloud Storage <img src=\"http://storage.googleapis.com/cloud-ml-data/img/flower_photos/sunflowers/1022552002_2b93faf9e7_n.jpg\" />"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jRFLfVjO62TJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The online prediction service expects images to be base64 encoded as described [here](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict#binary_data_in_prediction_input)."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "EzTZba9G62TM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "IMAGE_URL=gs://cloud-ml-data/img/flower_photos/sunflowers/1022552002_2b93faf9e7_n.jpg\n",
        "\n",
        "# Copy the image to local disk.\n",
        "gsutil cp $IMAGE_URL flower.jpg\n",
        "\n",
        "# Base64 encode and create request message in json format.\n",
        "python -c 'import base64, sys, json; img = base64.b64encode(open(\"flower.jpg\", \"rb\").read()); print json.dumps({\"image_bytes\":{\"b64\": img}}) ' &> request.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "pl6nWLFl62TP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Send it to the prediction service"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Wecr65Pi62TR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcloud ml-engine predict \\\n",
        "  --model=flowers \\\n",
        "  --version=${MODEL_TYPE} \\\n",
        "  --json-instances=./request.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6c9mNUpu62TT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "</pre>"
      ]
    }
  ]
}