{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_cloudmle_hparams.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vEeJQT4RLvQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning a TensorFlow model with Cloud ML Engine\n",
        "\n",
        "This notebook demonstrates how to configure a set of hyperparameter tuning experiments for a TensorFlow model and then use Cloud ML Engine to run a collection of parallel experiments (trials) to use a black-box Bayesian optimization algorithm to discover better perfoming hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "9fr0-m0OLvQU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "#@markdown Enter  your GCP Project ID:\n",
        "PROJECT = \"\" #@param {type: \"string\"}\n",
        "#@markdown Enter  your GCP Storage Bucket ID:\n",
        "BUCKET = \"\" #@param {type: \"string\"}\n",
        "#@markdown OPTIONAL: Replace with your GCP Storage Bucket Region:\n",
        "REGION = \"us-central1\\t\" #@param {type:\"string\"}\n",
        "\n",
        "MODEL_TYPE='cnn_batch_norm'       # convolutional neural network\n",
        "\n",
        "# do not change these\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['MODEL_TYPE'] = MODEL_TYPE\n",
        "os.environ['TFVERSION'] = '1.10'  # Tensorflow version\n",
        "\n",
        "def start_tensorboard(logdir, url_file):\n",
        "  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(logdir))\n",
        "  get_ipython().system_raw('lt --port 6006 >> {} 2>&1 &'.format(url_file))\n",
        "  get_ipython().system('cat {}'.format(url_file))\n",
        "\n",
        "def stop_tensorboard(url_file):\n",
        "  get_ipython().system_raw(\"ps -Af  | grep -E 'tensorboard|lt --port' | awk '{print $2}' | xargs -I % kill -9 %\")\n",
        "  get_ipython().system_raw(\"rm {}\".format(url_file))\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"Authenticated\")\n",
        "except:\n",
        "  print(\"Failed to authenticate\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pnvI4YTaLvQX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/osipov/training-data-analyst.git\n",
        "cp -r training-data-analyst/bootcamps/imagereco/fashionmodel .\n",
        "\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "USDJ8fRQLvQf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The hyperparam.yaml file specifies a goal for hyperparameter tuning as well as a range, type, and scale of hyperparameter values to explore. Cloud MLE uses  a smart search algorithm to discover better performing values within the specified constraints, in other words it does not try out every single value. The algorithm, based on Bayesian optimization, uses the information gained during search to adaptively choose hyperparameter values to explore.\n",
        "\n",
        "The following configuration uses a single trial to reduce the amount of time that hyperparameter tuning takes during the workshop. When using Cloud MLE in production, you will commonly use as many parallel trials as your GCP account quotas and project budget permit.\n",
        "\n",
        "Notice that in the following hyperparam.yaml file the number of the training steps is configured as a categorical value with 3 possible options. This is done just to illustrate how you can use categorical values for hyperparameter tuning. In practice, you could have used a linear or a log scale for the range of traning step parameters.\n"
      ]
    },
    {
      "metadata": {
        "id": "JgvR6u_Fqmm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile hyperparam.yaml\n",
        "trainingInput:\n",
        "  scaleTier: BASIC_GPU\n",
        "  hyperparameters:\n",
        "    goal: MAXIMIZE\n",
        "    maxTrials: 1\n",
        "    maxParallelTrials: 1\n",
        "    hyperparameterMetricTag: accuracy\n",
        "    params:\n",
        "    - parameterName: learning_rate\n",
        "      type: DOUBLE\n",
        "      minValue: 0.00001\n",
        "      maxValue: 0.01\n",
        "      scaleType: UNIT_LOG_SCALE      \n",
        "    - parameterName: train_batch_size\n",
        "      type: INTEGER\n",
        "      minValue: 128\n",
        "      maxValue: 1024\n",
        "      scaleType: UNIT_LOG_SCALE\n",
        "    - parameterName: train_steps\n",
        "      type: INTEGER\n",
        "      minValue: 400\n",
        "      maxValue: 4000\n",
        "      scaleType: UNIT_LOG_SCALE        \n",
        "    - parameterName: dprob\n",
        "      type: DOUBLE\n",
        "      minValue: 0.1\n",
        "      maxValue: 0.4\n",
        "      scaleType: UNIT_LINEAR_SCALE        \n",
        "    - parameterName: ksize1\n",
        "      type: CATEGORICAL\n",
        "      categoricalValues: [\"3\", \"5\", \"7\", \"11\"]  \n",
        "    - parameterName: ksize2\n",
        "      type: CATEGORICAL\n",
        "      categoricalValues: [\"3\", \"5\", \"7\", \"11\"]          \n",
        "    - parameterName: nfil1\n",
        "      type: CATEGORICAL\n",
        "      categoricalValues: [\"10\", \"15\", \"20\", \"25\"]  \n",
        "    - parameterName: nfil2\n",
        "      type: CATEGORICAL\n",
        "      categoricalValues: [\"10\", \"15\", \"20\", \"25\"]        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7qHQ1NzLvQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next, start a Cloud ML Engine hyperparameter tuning job on BASIC_GPU instances** using the hyperparam.yaml file for configuration.\n",
        "\n",
        "Notice that when starting a hyperparameter tuning job, the command line parameters that used to be provided via a command line (e.g. learning_rate) are omitted. Instead, the corresponding values will be provided by Cloud MLE for every trial of the hyperparameter tuning job."
      ]
    },
    {
      "metadata": {
        "id": "1GJ6qlJALvQn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/fashion/trained_${MODEL_TYPE}\n",
        "JOBNAME=fashion_${MODEL_TYPE}_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=${PWD}/fashionmodel/trainer \\\n",
        "   --job-dir=$OUTDIR \\\n",
        "   --staging-bucket=gs://$BUCKET \\\n",
        "   --scale-tier=BASIC_GPU \\\n",
        "   --runtime-version=$TFVERSION \\\n",
        "   --config=hyperparam.yaml \\\n",
        "   -- \\\n",
        "   --output_dir=$OUTDIR \\\n",
        "   --model=$MODEL_TYPE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KkMS5JN5zvMS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just like with the regular training jobs, you can monitor hyperparameter turning from the [Jobs](https://console.cloud.google.com/mlengine/jobs) section of the Cloud ML Engine service. Once the hyperparameter tuning job finishes, it should discover values for train_steps, learning_rate, and train_batch_size that can train a  model close to 88-89% accuracy."
      ]
    },
    {
      "metadata": {
        "id": "eWz3Po640xVX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Monitoring training with TensorBoard\n",
        "\n",
        "Models trained during hyperparameter tuning can also be monitored using TensorBoard. Go ahead and execute the next cell to launch tensorboard. Once you access the TensorBoard link, notice that the model(s) are prefixed with a number, e.g. 1/eval. Since the model.py uses the trial ID as a part of the model output directory, every hyperparameter tuned model will have a unique prefix in TensorBoard. This helps compare performance (e.g. accuracy) of different models in the same dashboard. You can experiment with training multiple models by changing the number of max trails in the hyperparam.yaml file earlier in this notebook and starting new hyperparameter tuning jobs. Be careful not to exceed your quota!"
      ]
    },
    {
      "metadata": {
        "id": "T1YdOC7bcLme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel\n",
        "start_tensorboard('gs://{}/fashion/trained_{}'.format(BUCKET, MODEL_TYPE), 'url')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dXyJzwDLvQw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%sx read -p 'Press Enter in the input box to stop TensorBoard '\n",
        "stop_tensorboard('url')\n",
        "print(\"Stopped\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ik-w03A6LvRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "</pre>"
      ]
    }
  ]
}