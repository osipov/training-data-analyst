{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_cnn_lab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-pfUJ-IEGMhW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab Exercise: Implement Convolutional and Maxpooling Layers for Fashion MNIST Image Classification\n",
        "\n",
        "In this notebook you will modify an implementation of a convolutional neural network to prepare it for training. After that you will validate the implementation by training it in your local environment."
      ]
    },
    {
      "metadata": {
        "id": "9zEJZeP9GMhX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# do not change these\n",
        "MODEL_TYPE='cnn'       # convolutional neural network\n",
        "os.environ['MODEL_TYPE'] = MODEL_TYPE\n",
        "\n",
        "def start_tensorboard(logdir, url_file):\n",
        "  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(logdir))\n",
        "  get_ipython().system_raw('lt --port 6006 >> {} 2>&1 &'.format(url_file))\n",
        "  get_ipython().system('cat {}'.format(url_file))\n",
        "\n",
        "def stop_tensorboard(url_file):\n",
        "  get_ipython().system_raw(\"ps -Af  | grep -E 'tensorboard|lt --port' | awk '{print $2}' | xargs -I % kill -9 %\")\n",
        "  get_ipython().system_raw(\"rm {}\".format(url_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zyNp1s4HGMhc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/osipov/training-data-analyst.git\n",
        "cp -r training-data-analyst/bootcamps/imagereco/fashionmodel .\n",
        "find fashionmodel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5XYIWgxGMhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Complete the TODOs before executing the next cell to train the model in your local environment**\n",
        "\n",
        "Modify the `model.py` file containing the convolutional neural network layer definitions in the `cnn_model` method per the instructions in the TODOs. Make sure to use the hyperparameter values specified by `nfil2` and `ksize2` variables. "
      ]
    },
    {
      "metadata": {
        "id": "ON-U7P2tGfQx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile fashionmodel/trainer/model.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# Licensed under the Apache License, Version 2.0 See footer for details.\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "HEIGHT=28\n",
        "WIDTH=28\n",
        "NCLASSES=10\n",
        "\n",
        "def cnn_model(img, mode, hparams):\n",
        "  ksize1 = hparams.get('ksize1', 5)\n",
        "  ksize2 = hparams.get('ksize2', 5)\n",
        "  nfil1 = hparams.get('nfil1', 10)\n",
        "  nfil2 = hparams.get('nfil2', 20)\n",
        "  dprob = hparams.get('dprob', 0.25)\n",
        "  \n",
        "  c1 = tf.layers.conv2d(img, filters=nfil1,\n",
        "                          kernel_size=ksize1, strides=1, # ?x28x28x10\n",
        "                          padding='same', activation=tf.nn.relu)\n",
        "  p1 = tf.layers.max_pooling2d(c1,pool_size=2, strides=2) # ?x14x14x10\n",
        "\n",
        "  #TODO: apply a second convolution to the output of p1\n",
        "  #make sure to use the hyperparameter values of ksize2 and nfil2\n",
        "  #also use the same strides, padding, and activation values\n",
        "  #as in the previous convolutional layer\n",
        "  \n",
        "  #TODO: apply a pooling layer with pool_size=2 and strides=2\n",
        "  \n",
        "  outlen = p2.shape[1]*p2.shape[2]*p2.shape[3] #outlen should be 980\n",
        "  p2flat = tf.reshape(p2, [-1, outlen]) # flattened\n",
        "  \n",
        "  h3 = tf.layers.dense(p2flat, 300, activation=tf.nn.relu) \n",
        "  ylogits = tf.layers.dense(h3, NCLASSES, activation=None)\n",
        "\n",
        "  return ylogits, NCLASSES\n",
        "\n",
        "def dnn_model(img, mode, hparams):\n",
        "  X = tf.reshape(img, [-1, HEIGHT*WIDTH]) #flatten\n",
        "  h1 = tf.layers.dense(X, 100, activation=tf.nn.relu)\n",
        "  h2 = tf.layers.dense(h1, 100, activation=tf.nn.relu)\n",
        "  h3 = tf.layers.dense(h2, 100, activation=tf.nn.relu)\n",
        "  ylogits = tf.layers.dense(h3, NCLASSES, activation=None)\n",
        "  return ylogits, NCLASSES\n",
        "\n",
        "def serving_input_fn():\n",
        "    #input will be rank 3\n",
        "    feature_placeholders = {\n",
        "        'image': tf.placeholder(tf.float32, [None, HEIGHT, WIDTH])}\n",
        "    #but model function requires rank 4\n",
        "    features = {\n",
        "        'image': tf.expand_dims(feature_placeholders['image'],-1)} \n",
        "    return tf.estimator.export.ServingInputReceiver(features, \n",
        "                                                    feature_placeholders)\n",
        "\n",
        "def train_and_evaluate(output_dir, hparams):\n",
        "  EVAL_INTERVAL = 60\n",
        "\n",
        "  (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "  train_images = train_images.astype(dtype=np.float32, copy=False).reshape(train_images.shape + (1,))\n",
        "  test_images = test_images.astype(dtype=np.float32, copy=False).reshape(test_images.shape + (1,))\n",
        "  train_labels = np.eye(10)[train_labels]\n",
        "  test_labels = np.eye(10)[test_labels]    \n",
        "    \n",
        "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'image': train_images},\n",
        "    y=train_labels,\n",
        "    batch_size=100,\n",
        "    num_epochs=None,\n",
        "    shuffle=True,\n",
        "    queue_capacity=5000\n",
        "  )\n",
        "\n",
        "  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'image':test_images},\n",
        "    y=test_labels,\n",
        "    batch_size=100,\n",
        "    num_epochs=1,\n",
        "    shuffle=False,\n",
        "    queue_capacity=5000\n",
        "  )\n",
        "  estimator = tf.estimator.Estimator(model_fn = image_classifier,\n",
        "                                     params = hparams,\n",
        "                                     config=tf.estimator.RunConfig(\n",
        "                                         save_checkpoints_secs = EVAL_INTERVAL),\n",
        "                                     model_dir = output_dir)\n",
        "  train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn,\n",
        "                                    max_steps = hparams['train_steps'])\n",
        "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn,\n",
        "                                  steps = None,\n",
        "                                  exporters = exporter,\n",
        "                                  throttle_secs = EVAL_INTERVAL)\n",
        "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "  \n",
        "\n",
        "def image_classifier(features, labels, mode, params):\n",
        "  model_functions = {\n",
        "      'dnn':dnn_model,\n",
        "      'cnn':cnn_model}\n",
        "  \n",
        "  model_function = model_functions[params['model']]  \n",
        "  ylogits, nclasses = model_function(features['image'], mode, params)\n",
        "\n",
        "  probabilities = tf.nn.softmax(ylogits)\n",
        "  classes = tf.cast(tf.argmax(probabilities, 1), tf.uint8)\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            logits=ylogits, labels=labels))\n",
        "    evalmetrics = {'accuracy': tf.metrics.accuracy(classes, tf.argmax(labels, 1))}\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      # this is needed for batch normalization, but has no effect otherwise\n",
        "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "      with tf.control_dependencies(update_ops):\n",
        "         train_op = tf.contrib.layers.optimize_loss(\n",
        "             loss, \n",
        "             tf.train.get_global_step(),\n",
        "             learning_rate=params['learning_rate'], \n",
        "             optimizer=\"Adam\")\n",
        "    else:\n",
        "      train_op = None\n",
        "  else:\n",
        "    loss = None\n",
        "    train_op = None\n",
        "    evalmetrics = None\n",
        " \n",
        "  return tf.estimator.EstimatorSpec(\n",
        "        mode=mode,\n",
        "        predictions={\"probabilities\": probabilities, \"classes\": classes},\n",
        "        loss=loss,\n",
        "        train_op=train_op,\n",
        "        eval_metric_ops=evalmetrics,\n",
        "        export_outputs={'classes':tf.estimator.export.PredictOutput(\n",
        "            {\"probabilities\": probabilities, \"classes\": classes})}\n",
        "    )\n",
        "  \n",
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBXrbkN3GMhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run as a Python module"
      ]
    },
    {
      "metadata": {
        "id": "5wf6KTFqGMhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf fashion_trained\n",
        "export PYTHONPATH=${PYTHONPATH}:${PWD}/fashionmodel\n",
        "python -m trainer.task \\\n",
        "   --output_dir=${PWD}/fashion_trained/${MODEL_TYPE} \\\n",
        "   --train_steps=600 \\\n",
        "   --learning_rate=0.01 \\\n",
        "   --train_batch_size=100 \\\n",
        "   --model=$MODEL_TYPE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_z5Lxr_GMhn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Monitoring training with TensorBoard\n",
        "\n",
        "Run the next cell to launch tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "aRba0EHTGMho",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel\n",
        "start_tensorboard('./fashion_trained/{}'.format(MODEL_TYPE), 'url')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E16Iuo4SGMhq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%sx read -p 'Press Enter in the input box to stop TensorBoard '\n",
        "stop_tensorboard('url')\n",
        "print(\"Stopped\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfTTzG6VGMiC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "# Copyright 2017 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "</pre>"
      ]
    }
  ]
}