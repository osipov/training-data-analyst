{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_datasets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PDDqFd8hon3v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Create and benchmark train, validation, and test datasets </h1>\n",
        "\n",
        "In this notebook, you will prepare the training, validation (sometimes called evaluation), and test datasets using the NYC taxi fare data. After that, you will benchmark against these datasets, in other words use your benchmark model to calculate the metric values for the training, validation, and test examples. In parallel, you will learn how to include various bash shell commands (e.g. ls, head, and others) in your Colab notebooks.\n",
        "\n",
        "\n",
        "---\n",
        "Before you start, **make sure that you are logged in with your student account**. Otherwise you may incur Google Cloud charges for using this notebook. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "Wq75B91eon3y",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import shutil\n",
        "from google.cloud import bigquery\n",
        "\n",
        "#@markdown Copy-paste your GCP Project ID in the following field:\n",
        "\n",
        "\n",
        "PROJECT = \"\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown Next, use Shift-Enter to run this cell and complete authentication.\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"AUTHENTICATED\")\n",
        "except:\n",
        "  print(\"FAILED to authenticate\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quQPY9r8MzQ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, query for 1 out of 100,000 rows of the entire taxi fare dataset and apply the clean up pre-processing rules you have developed in the earlier lab. Based on the summary from the Pandas describe table, you can confirm that there are roughly 10,500 rows of cleaned-up data in the `tripsqc` variable."
      ]
    },
    {
      "metadata": {
        "id": "YdiXbfDuI-LI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EVERY_N = 100000\n",
        "bq = bigquery.Client(project=PROJECT)\n",
        "\n",
        "trips = bq.query('''\n",
        "  SELECT\n",
        "    pickup_datetime,\n",
        "    pickup_longitude, pickup_latitude, \n",
        "    dropoff_longitude, dropoff_latitude,\n",
        "    passenger_count,\n",
        "    trip_distance,\n",
        "    tolls_amount,\n",
        "    fare_amount,\n",
        "    total_amount\n",
        "  FROM\n",
        "    `nyc-tlc.yellow.trips`\n",
        "  WHERE\n",
        "    MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), %d) = 1\n",
        "    \n",
        "    #note that that trips with zero distance or \n",
        "    #costing less than $2.50 are excluded    \n",
        "    AND trip_distance > 0 AND fare_amount >= 2.5    \n",
        "    \n",
        "''' % (EVERY_N)).to_dataframe()\n",
        "\n",
        "def preprocess(trips_in):\n",
        "  trips = trips_in.copy(deep=True)\n",
        "  trips.fare_amount = trips.fare_amount + trips.tolls_amount\n",
        "  del trips['tolls_amount']\n",
        "  del trips['total_amount']\n",
        "  del trips['trip_distance']\n",
        "  del trips['pickup_datetime']\n",
        "  qc = np.all([\\\n",
        "             trips['pickup_longitude'] > -78, \\\n",
        "             trips['pickup_longitude'] < -70, \\\n",
        "             trips['dropoff_longitude'] > -78, \\\n",
        "             trips['dropoff_longitude'] < -70, \\\n",
        "             trips['pickup_latitude'] > 37, \\\n",
        "             trips['pickup_latitude'] < 45, \\\n",
        "             trips['dropoff_latitude'] > 37, \\\n",
        "             trips['dropoff_latitude'] < 45, \\\n",
        "             trips['passenger_count'] > 0,\n",
        "            ], axis=0)\n",
        "  return trips[qc]\n",
        "\n",
        "tripsqc = preprocess(trips)\n",
        "tripsqc.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbsRzcqLon46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Create ML datasets </h3>\n",
        "\n",
        "The next cell splits the cleaned up dataset randomly into training, validation and test sets. Since you are working with an in-memory dataset (for now), you will use a 70%-15%-15% split. Later you will learn about the benefits of allocating a larger percentage of the entire dataset for training."
      ]
    },
    {
      "metadata": {
        "id": "9WZgUiO0on47",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled = tripsqc.sample(frac=1)\n",
        "trainsize = int(len(shuffled['fare_amount']) * 0.70)\n",
        "validsize = int(len(shuffled['fare_amount']) * 0.15)\n",
        "\n",
        "df_train = shuffled.iloc[:trainsize, :]\n",
        "df_valid = shuffled.iloc[trainsize:(trainsize+validsize), :]\n",
        "df_test = shuffled.iloc[(trainsize+validsize):, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sWZ_UdqVon4-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G60adT61on5C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_valid.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1i2mo1CAon5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ng1gbvDuon5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's write out the three dataframes to appropriately named csv files. The files will be useful for local training while you are developing your machine learning models. In future labs, you will scale out to a larger dataset using other serverless capabilities like Cloud Machine Learning Engine (Cloud MLE) and Dataflow."
      ]
    },
    {
      "metadata": {
        "id": "yuQwBVVpon5L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_csv(df, filename):\n",
        "  outdf = df.copy(deep=False)\n",
        "  outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
        "  # reorder columns so that target is first column\n",
        "  cols = outdf.columns.tolist()\n",
        "  cols.remove('fare_amount')\n",
        "  cols.insert(0, 'fare_amount')\n",
        "  print (cols)  # new order of columns\n",
        "  outdf = outdf[cols]\n",
        "  outdf.to_csv(filename, header=False, index_label=False, index=False)\n",
        "\n",
        "to_csv(df_train, 'taxi-train.csv')\n",
        "to_csv(df_valid, 'taxi-valid.csv')\n",
        "to_csv(df_test, 'taxi-test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MOPzJGT7OtWu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are 2 ways to execute shell commands in the OS environment hosting this notebook:\n",
        "\n",
        "1. You can prefix your `bash` command with an exclaimation mark as shown in the next cell.\n",
        "\n",
        "2. You can use the `%%bash` \"magic\" as the first line of a code cell. This approach is better suited for multi-line shell scripts.\n",
        "\n",
        "If you are interested in details about Jupyter \"magics\", you can learn more [here](https://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "2msQdkagon5Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head -10 taxi-valid.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RGZsBGRcon5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Verify that datasets exist </h3>"
      ]
    },
    {
      "metadata": {
        "id": "57XAd4O8on5U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l *.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1AvPQIDon5Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are 3 .csv files corresponding to train, valid, test datasets.  The ratio of file sizes reflect the percentages in the split of the data."
      ]
    },
    {
      "metadata": {
        "id": "Tcqtnc0Hon5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "head taxi-train.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "St5NHo1lon5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks good! Now our datasets are prepared so you will be ready to train machine learning models, validate them and evaluate them."
      ]
    },
    {
      "metadata": {
        "id": "DBbYTPZAon5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Benchmark </h3>\n",
        "\n",
        "Before committing to a complex machine learning model, it is a good idea to come up with a very simple, heuristic model and use that as a benchmark.\n",
        "\n",
        "My model is going to simply divide the mean fare_amount by the mean trip_distance to come up with an average rate per kilometer and use that to predict.  Let's compute the RMSE of such a model."
      ]
    },
    {
      "metadata": {
        "id": "o4TuDR8Jon5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distance_between(lat1, lon1, lat2, lon2):\n",
        "  # haversine formula to compute distance \"as the crow flies\".  Taxis can't fly of course.\n",
        "  dist = np.degrees(np.arccos(np.minimum(1,np.sin(np.radians(lat1)) * np.sin(np.radians(lat2)) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.cos(np.radians(lon2 - lon1))))) * 60 * 1.515 * 1.609344\n",
        "  return dist\n",
        "\n",
        "def estimate_distance(df):\n",
        "  return distance_between(df['pickuplat'], df['pickuplon'], df['dropofflat'], df['dropofflon'])\n",
        "\n",
        "def compute_rmse(actual, predicted):\n",
        "  return np.sqrt(np.mean((actual-predicted)**2))\n",
        "\n",
        "def print_rmse(df, rate, name):\n",
        "  print (\"{1} RMSE = {0}\".format(compute_rmse(df['fare_amount'], rate*estimate_distance(df)), name))\n",
        "\n",
        "FEATURES = ['pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
        "TARGET = 'fare_amount'\n",
        "columns = list([TARGET])\n",
        "columns.extend(FEATURES) # in CSV, target is the first column, after the features\n",
        "columns.append('key')\n",
        "\n",
        "df_train = pd.read_csv('taxi-train.csv', header=None, names=columns)\n",
        "df_valid = pd.read_csv('taxi-valid.csv', header=None, names=columns)\n",
        "df_test = pd.read_csv('taxi-test.csv', header=None, names=columns)\n",
        "\n",
        "rate = df_train['fare_amount'].mean() / estimate_distance(df_train).mean()\n",
        "print (\"Rate = ${0}/km\".format(rate))\n",
        "\n",
        "print_rmse(df_train, rate, 'Train')\n",
        "print_rmse(df_valid, rate, 'Valid') \n",
        "print_rmse(df_test, rate, 'Test') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5TGIWkQon5i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Benchmark on same dataset</h2>\n",
        "\n",
        "The RMSE depends on the dataset and for meaningful and reproducible comparisons it is critical to measure on the same dataset each time. The following query will continue to be reused in the later labs:"
      ]
    },
    {
      "metadata": {
        "id": "O9xVQv49on5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_query(phase, EVERY_N):\n",
        "  \"\"\"\n",
        "  phase: 1=train 2=valid\n",
        "  \"\"\"\n",
        "  base_query = \"\"\"\n",
        "    SELECT\n",
        "      (tolls_amount + fare_amount) AS fare_amount,\n",
        "      \n",
        "      CONCAT( STRING(pickup_datetime), \n",
        "              CAST(pickup_longitude AS STRING), \n",
        "              CAST(pickup_latitude AS STRING),\n",
        "              CAST(dropoff_latitude AS STRING), \n",
        "              CAST(dropoff_longitude AS STRING)) AS key,\n",
        "              \n",
        "      EXTRACT(DAYOFWEEK FROM pickup_datetime)*1.0 AS dayofweek,\n",
        "      EXTRACT(HOUR FROM pickup_datetime)*1.0 AS hourofday,\n",
        "      pickup_longitude AS pickuplon,\n",
        "      pickup_latitude AS pickuplat,\n",
        "      dropoff_longitude AS dropofflon,\n",
        "      dropoff_latitude AS dropofflat,\n",
        "      passenger_count*1.0 AS passengers\n",
        "    FROM\n",
        "      `nyc-tlc.yellow.trips`\n",
        "    WHERE\n",
        "      {}\n",
        "      AND trip_distance > 0\n",
        "      AND fare_amount >= 2.5\n",
        "      AND pickup_longitude > -78\n",
        "      AND pickup_longitude < -70\n",
        "      AND dropoff_longitude > -78\n",
        "      AND dropoff_longitude < -70\n",
        "      AND pickup_latitude > 37\n",
        "      AND pickup_latitude < 45\n",
        "      AND dropoff_latitude > 37\n",
        "      AND dropoff_latitude < 45\n",
        "      AND passenger_count > 0\n",
        "  \"\"\"\n",
        "  if EVERY_N == None:\n",
        "    if phase < 2:\n",
        "      # training\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) < 2\"\n",
        "    else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) = 2\"\n",
        "  else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), %d) = %d\" % (EVERY_N, phase)\n",
        "    \n",
        "  query = base_query.format(selector)\n",
        "\n",
        "  return query\n",
        "\n",
        "sql = create_query(2, 100000)\n",
        "df_valid = bq.query(sql).to_dataframe()\n",
        "print_rmse(df_valid, 2.56, 'Final Validation Set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MT9dwy_ton5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The simple distance-based rule gives us a RMSE of <b>$7.42</b>.  We have to beat this, of course, but you will find that simple rules of thumb like this can be surprisingly difficult to beat.\n",
        "\n",
        "Let's be ambitious, though, and make our goal to build ML models that have a RMSE of less than $6 on the test set."
      ]
    },
    {
      "metadata": {
        "id": "HVuX8oeMon5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2016 Counter Factual .AI\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}