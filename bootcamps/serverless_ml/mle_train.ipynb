{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mle_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "AHJIrdDaUxaA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Scaling up training using Cloud ML Engine </h1>\n",
        "\n",
        "In this notebook, you will take a previously developed TensorFlow model to predict taxifare rides and package it up so that it can be run on Cloud Machine Learning Engine (MLE). For now, the model will be trained on a small dataset. The model is still rather simplistic, and therefore, the accuracy of the model is not great either.  However, this notebook illustrates *how* to package up a TensorFlow model to run it within Cloud MLE. \n",
        "\n",
        "Later in the course, you will look at ways to make a more effective machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Before you start, **make sure that you are logged in with your student account**. Otherwise you may incur Google Cloud charges for using this notebook. \n",
        "\n",
        "---\n",
        "\n",
        "Also, remember to uncheck \"Reset all runtimes before running\" when executing the next cell.\n",
        "\n",
        "Reseting the runtime will delete any files you may have on your notebook file system. \n",
        "\n",
        "![](https://i.imgur.com/9dgw0h0.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "DQXV2gT2VJKb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@markdown Copy-paste your GCP Project ID in the following field:\n",
        "\n",
        "PROJECT = \"\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown Next, use Shift-Enter to run this cell and complete authentication.\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"AUTHENTICATED\")\n",
        "except:\n",
        "  print(\"FAILED to authenticate\")\n",
        "\n",
        "#Modify the following to use a different bucket and/or region\n",
        "#for Google Cloud Storage and for Cloud MLE\n",
        "BUCKET = PROJECT  \n",
        "REGION = \"us-central1\"  \n",
        "\n",
        "# Copy taxi-*.csv files from github if they are missing from the runtime.\n",
        "!wget --quiet -nc https://github.com/osipov/training-data-analyst/raw/master/bootcamps/serverless_ml/taxi-11k-datasets.zip\n",
        "!unzip -q -n taxi-11k-datasets.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0MQ_B40UxaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Environment variables</h2>\n",
        "\n",
        "The previous code cell initialized the Python runtime with values for your project ID, Google Cloud Storage bucket ID, and a Google Cloud region where your data is stored and processed by Cloud MLE jobs.\n",
        "\n",
        "The next code cell takes these default values and uses Python `os` library to create bash shell environment variables for the project, bucket, region and the TensorFlow version. This is needed so you can continue using the default values for these variables in the bash scripts later in this notebook."
      ]
    },
    {
      "metadata": {
        "id": "-J7URkLnUxaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for bash\n",
        "import os\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['TF_VERSION'] = '1.12'  # Cloud MLE Latest supported Tensorflow version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5h-fxyv1hRam",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Colab includes the latest version of [Google Cloud SDK](https://cloud.google.com/sdk) which provides a set of management tools for a Google Cloud account. One of the tools in the SDK is called `gcloud` and it is a general purpose tool for managing almost all services on Google Cloud. The next code cell uses `gcloud` to configure the shell environment to use your GCP Project ID and the default Google Cloud region."
      ]
    },
    {
      "metadata": {
        "id": "uyB9sL6iUxaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01sFKqP-UxaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The default security settings of Google Cloud do not allow Cloud MLE (or any other service) to access private data stored in your storage bucket.  Next, you will run a standard script that lets Cloud MLE to read and write data in your project's bucket. This will ensure that MLE can read training, validation, and test data from the bucket and also write checkpoints of a trained TensorFlow model to the bucket."
      ]
    },
    {
      "metadata": {
        "id": "zm0851LcUxaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "PROJECT_ID=$PROJECT\n",
        "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
        "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
        "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
        "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
        "    print response['serviceAccount']\")\n",
        "\n",
        "echo \"Authorizing the Cloud ML Service $SVC_ACCOUNT to access files in $BUCKET\"\n",
        "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
        "\n",
        "echo \"NOTE: the following CommandException (No URLs matched if bucket is empty) can be ignored\"\n",
        "gsutil -q -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
        "\n",
        "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXGdK8-BUxaQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Don't worry if you see an exception message about `No URLs matched` in the previous cell. It just means that the bucket was empty when the script ran.\n",
        "\n",
        "<h2> Packaging up the code </h2>\n",
        "\n",
        "The script takes the code from github and organizes it into a standard Python package structure. The model is now located in `model.py` while the main entry point into the model is in `task.py` The `find` command at the end of the script shows the details of the directory structure for the Python package. This matches what you learned earlier about packaging TensorFlow models for Cloud MLE."
      ]
    },
    {
      "metadata": {
        "id": "aUcGDecYH7Vm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf taxifare\n",
        "mkdir -p taxifare/trainer\n",
        "\n",
        "for file in taxifare/setup.py \\\n",
        "            taxifare/trainer/__init__.py \\\n",
        "            taxifare/trainer/model.py \\\n",
        "            taxifare/trainer/task.py\n",
        "do\n",
        "  wget --quiet -nc \\\n",
        "  https://github.com/osipov/training-data-analyst/raw/master/bootcamps/serverless_ml/cloudmle/$file \\\n",
        "  -O $file\n",
        "done\n",
        "\n",
        "find taxifare"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GhQf19OGlsA3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take a few minutes to confirm that `model.py` contains the code you used earlier...\n"
      ]
    },
    {
      "metadata": {
        "id": "hCPhkJr1l2Ok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat taxifare/trainer/model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u189_49Hl4-i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "... and that code in `task.py` isn't too suprising."
      ]
    },
    {
      "metadata": {
        "id": "T3EA-KuhmDuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat taxifare/trainer/task.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m0ZBdVHzUxbO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Submit training job using gcloud </h2>\n",
        "\n",
        "Copy the training data from the Colab file system to the storage bucket:"
      ]
    },
    {
      "metadata": {
        "id": "xlQ-xbXEUxbP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo $BUCKET\n",
        "gsutil -m rm -rf gs://${BUCKET}/taxifare/11k/*.csv\n",
        "gsutil -m cp ${PWD}/*.csv gs://${BUCKET}/taxifare/11k/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCZRBpsFmTrq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Don't worry if in the previous code cell you see a message about files/objects that could not be removed. This message occurs because `gsutil rm` command tries to clean up the  directory for the training, validation, and test csv files. The `gsutil` command is another tool from the Google Cloud SDK. It used to manage storage buckets and to move data to and from buckets over the network.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jSi2gT7aoG5l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the next cell, submit a training job to Cloud MLE using the `gcloud` command. Notice that the command is using the Python package in the `taxifare/trainer` directory of your Colab environment. In contrast, the training and validation data files are sourced from the storage bucket. The `OUTDIR` variable specifies the location in the storage bucket where  MLE will store model checkpoint files. In the upcoming notebooks you will use the trained model from that location."
      ]
    },
    {
      "metadata": {
        "id": "uZ2OCmMQUxbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/taxifare/11k/taxi_trained\n",
        "\n",
        "JOBNAME=mle_train_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=${PWD}/taxifare/trainer \\\n",
        "   --job-dir=$OUTDIR \\\n",
        "   --staging-bucket=gs://$BUCKET \\\n",
        "   --scale-tier=BASIC \\\n",
        "   --runtime-version=${TF_VERSION} \\\n",
        "   -- \\\n",
        "   --train_data_paths=\"gs://${BUCKET}/taxifare/11k/taxi-train*\" \\\n",
        "   --eval_data_paths=\"gs://${BUCKET}/taxifare/11k/taxi-valid*\"  \\\n",
        "   --output_dir=$OUTDIR \\\n",
        "   --train_steps=10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AALC5kqumg-n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After you submit the job you should see a message confirming that your job was QUEUED. To monitor the progress of the job from the GCP user interface, navigate to [Jobs](https://console.cloud.google.com/mlengine/jobs) part of the Cloud ML Engine service. Use the \"View Logs\" link to get the details. In the upcoming lab, you will also monitor training details using TensorBoard."
      ]
    },
    {
      "metadata": {
        "id": "bIrvxhzPwtLf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Recap</h2>\n",
        "\n",
        "In this notebook, you configured security permissions to allow Cloud MLE to access files on your Google Cloud Storage bucket. Recall that you are using the bucket to support scalability to very large (petabyte sized) datasets. \n",
        "\n",
        "Next, you packaged the TensorFlow code in the taxifare directory, following Python packaging conventions. You checked that `task.py` file is the main entrypoint into your model and its code passes various parameters to your model.\n",
        "\n",
        "Finally, you copied the training and validation csv files to your storage bucket, launched the training process, and viewed the logs to confirm that the training started.\n",
        "\n",
        "<b>Don't wait for training to finish!</b> If you confirmed that the training job is started, you're done with this lab and are ready to continue with the rest of this session!"
      ]
    },
    {
      "metadata": {
        "id": "yLup-4j7Uxbg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Counter Factual .AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}