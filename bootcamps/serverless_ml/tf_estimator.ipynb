{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_estimator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hHC3xZ8jTlMS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Machine Learning using tf.estimator </h1>\n",
        "\n",
        "In this notebook, you will create a machine learning model using tf.estimator API and evaluate the model's performance. In this lab, the training dataset is small enough (roughly 7300 training examples), so it wil fit in-memory. This means that the data can be passed as a Pandas Dataframe to the machine learning model.\n",
        "\n",
        "Note that to train and validate the machine learning model, this notebook tries to use the `taxi-train.csv` and `taxi-valid.csv` files from an earlier lab. You can check that you still have those files by going to **View > Table of Contents** in the menu bar and then choosing the **Files ** tab as shown on the following screenshot. Try clicking on the Refresh button in the Files tab if you don't see them.\n",
        "\n",
        "Don't worry if you are having trouble finding those files. It is possible that they were automatically deleted if your Colab runtime was restarted after you worked on the earlier lab which created the files based on the SQL query against the data warehouse.  The next cell of the notebook will check if your runtime is missing the `taxi-*.csv` files and if so, will try to download them from github. Alternatively, you can go back to the earlier lab available [here](bit.ly/d1-create-datasets) and re-create the files.\n",
        "\n",
        "![](https://i.imgur.com/MP0EKTk.png)\n",
        "\n",
        "\n",
        "---\n",
        "Before you start, **make sure that you are logged in with your student account**. Otherwise you may incur Google Cloud charges for using this notebook. \n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "I2RG5e7rTlMT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "#@markdown Remember to uncheck \"Reset all runtimes before running\"\n",
        "\n",
        "#@markdown As you know, reseting the runtime will delete any files you may have on your notebook file system. \n",
        "#@markdown ![](https://i.imgur.com/9dgw0h0.png)\n",
        "\n",
        "#@markdown Enter  your GCP Project ID:\n",
        "PROJECT = \"osipov-archive-backup-coldline\" #@param {type: \"string\"}\n",
        "#@markdown Next, use Shift-Enter to run this cell and complete authentication.\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"AUTHENTICATED\")\n",
        "except:\n",
        "  print(\"FAILED to authenticate\")\n",
        "  \n",
        "bq = bigquery.Client(project=PROJECT)\n",
        "  \n",
        "print tf.__version__\n",
        "\n",
        "# Copy taxi-*.csv files from github if they are missing from the runtime.\n",
        "!wget -nc https://github.com/osipov/training-data-analyst/raw/master/bootcamps/serverless_ml/taxi-11k-datasets.zip  \n",
        "!unzip -n taxi-11k-datasets.zip  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AccJ3uzCTlMV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here you prepare Pandas dataframes to store the training and validation examples. There are roughly 7300 and 1600 data points in the training and validation example datasets respectively so they easily fit in memory.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "oEjosBQKTlMW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# In CSV, label is the first column, after the features, followed by the key\n",
        "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
        "FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\n",
        "LABEL = CSV_COLUMNS[0]\n",
        "\n",
        "df_train = pd.read_csv('./taxi-train.csv', header = None, names = CSV_COLUMNS)\n",
        "df_valid = pd.read_csv('./taxi-valid.csv', header = None, names = CSV_COLUMNS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IK3FojscrcAJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3CmIvw3IreCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_valid.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "HD3OMSnoTlMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Input function to read from Pandas Dataframe into tf.constant </h2>"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jMVYjpObTlMc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_input_fn(df, num_epochs):\n",
        "  return tf.estimator.inputs.pandas_input_fn(\n",
        "    x = df,\n",
        "    y = df[LABEL],\n",
        "    batch_size = 128,\n",
        "    num_epochs = num_epochs,\n",
        "    shuffle = True,\n",
        "    queue_capacity = 1000,\n",
        "    num_threads = 1\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hf0fy0v6TlMe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create feature columns for estimator"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MpxBbVGrTlMf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_feature_cols():\n",
        "  input_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
        "  return input_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5zT1tnU0TlMh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Linear Regression with tf.estimator  </h3>"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vzGivhzQTlMh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "OUTDIR = 'taxi_trained'\n",
        "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
        "\n",
        "model = tf.estimator.LinearRegressor(\n",
        "      feature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
        "\n",
        "model.train(input_fn = make_input_fn(df_train, num_epochs = 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UpPP9p-kTlMk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluate on the validation data. In general, you should not look at the test data until after you have released a model."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cRK8J_-tTlMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_rmse(model, name, df):\n",
        "  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n",
        "  print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss']))\n",
        "print_rmse(model, 'validation', df_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NCe3syqzTlMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is nowhere near our benchmark metric (RMSE of $6 or so on this data), but it serves to demonstrate what TensorFlow code looks like.  Let's use this model for prediction."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jiJOdwgpTlMq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Read saved model and use it for prediction\n",
        "model = tf.estimator.LinearRegressor(\n",
        "      feature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
        "preds_iter = model.predict(input_fn = make_input_fn(df_valid, 1))\n",
        "print [pred['predictions'][0] for pred in list(itertools.islice(preds_iter, 5))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-T4tg09sTlMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This explains why the RMSE was so high -- the model essentially predicts the same amount for every trip.  Would a more complex model help? Let's try using a deep neural network.  The tf.estimator API makes it quite straightforward."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MB3YtZTtTlMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Deep Neural Network regression </h3>"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hAjItrMlTlMz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
        "\n",
        "#notice the use of the DNNRegressor model\n",
        "model = tf.estimator.DNNRegressor(hidden_units = [32, 8, 2],\n",
        "      feature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
        "\n",
        "model.train(input_fn = make_input_fn(df_train, num_epochs = 100));\n",
        "print_rmse(model, 'validation', df_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hh3QnDIsTlM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So what was the point of spending all this time learning about deep neural networks if they can't even beat the naive benchmark model?  Well, you started using TensorFlow for Machine Learning, but you have not yet learned how to do it well.  That's what the rest of this session is about!"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IuQspm5uTlM2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Benchmark dataset </h2>\n",
        "\n",
        "Remember that if you had to choose between two models you should choose the one with the lower validation error. Next, you should measure the metric on the test data with the selected model. Let's start the process using the dataset in the data warehouse."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "hCe-HmvwTlM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_query(phase, EVERY_N):\n",
        "  \"\"\"\n",
        "  phase: 1=train 2=valid\n",
        "  \"\"\"\n",
        "  base_query = \"\"\"\n",
        "    SELECT\n",
        "      (tolls_amount + fare_amount) AS fare_amount,\n",
        "      \n",
        "      CONCAT( STRING(pickup_datetime), \n",
        "              CAST(pickup_longitude AS STRING), \n",
        "              CAST(pickup_latitude AS STRING),\n",
        "              CAST(dropoff_latitude AS STRING), \n",
        "              CAST(dropoff_longitude AS STRING)) AS key,\n",
        "\n",
        "      EXTRACT(DAYOFWEEK FROM pickup_datetime)*1.0 AS dayofweek,\n",
        "      EXTRACT(HOUR FROM pickup_datetime)*1.0 AS hourofday,\n",
        "      pickup_longitude AS pickuplon,\n",
        "      pickup_latitude AS pickuplat,\n",
        "      dropoff_longitude AS dropofflon,\n",
        "      dropoff_latitude AS dropofflat,\n",
        "      passenger_count*1.0 AS passengers\n",
        "    FROM\n",
        "      `nyc-tlc.yellow.trips`\n",
        "    WHERE\n",
        "      {}\n",
        "      AND trip_distance > 0\n",
        "      AND fare_amount >= 2.5\n",
        "      AND pickup_longitude > -78\n",
        "      AND pickup_longitude < -70\n",
        "      AND dropoff_longitude > -78\n",
        "      AND dropoff_longitude < -70\n",
        "      AND pickup_latitude > 37\n",
        "      AND pickup_latitude < 45\n",
        "      AND dropoff_latitude > 37\n",
        "      AND dropoff_latitude < 45\n",
        "      AND passenger_count > 0\n",
        "  \"\"\"\n",
        "  if EVERY_N == None:\n",
        "    if phase < 2:\n",
        "      # training\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) < 2\"\n",
        "    else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) = 2\"\n",
        "  else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), %d) = %d\" % (EVERY_N, phase)\n",
        "    \n",
        "  query = base_query.format(selector)\n",
        "\n",
        "  return query\n",
        "\n",
        "sql = create_query(2, 100000)\n",
        "df = bq.query(sql).to_dataframe()\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "52WuBhfvTlM5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_rmse(model, 'benchmark', df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "z5VZaBCJTlM8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "RMSE on the validation dataset is <b>10.6</b> (your results will vary because of random seeds).\n",
        "\n",
        "This is not only way more than our original benchmark of 6.00, but it doesn't even beat our distance-based rule's RMSE of 8.02.\n",
        "\n",
        "<h3>Recap</h3>\n",
        "\n",
        "In this notebook you learned how to write a simple TensorFlow model but you are yet to make your model performant. Remember that the current implementation assumes that the training and validation datasets fit in memory of the node where you are executing your code. Next, you will learn how to scale up your model implementation to support large datasets that can range to petabytes of data."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_8UV2claTlM9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2017 Counter Factual .AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}