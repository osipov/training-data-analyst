{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batched.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bbs_nh8DobIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Refactor to support out-of-memory data and custom features</h1>\n",
        "\n",
        "In this notebook, you will continue using the same small development dataset (about 11.5k rows), but the TensorFlow implementation will change to:\n",
        "<ol>\n",
        "  <li> Read data in batches </li>\n",
        "  <li> Support shared or distributed filesystems, like Google Cloud Storage, HDFS, AWS S3</li>\n",
        "  <li> Enable custom features based on individual rows from data</li>\n",
        "</ol>\n",
        "The Pandas function in the previous notebook also batched, only after it had read the entire dataset into memory. With a large dataset, this isn't an option.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Before you start, **make sure that you are logged in with your student account**. Otherwise you may incur Google Cloud charges for using this notebook. \n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bIif0vZKobIo",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "6c6e003c-cc90-4642-a199-74427b4f4473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "print tf.__version__\n",
        "\n",
        "#@markdown Remember to uncheck \"Reset all runtimes before running\"\n",
        "\n",
        "#@markdown As you know, reseting the runtime will delete any files you may have on your notebook file system. \n",
        "#@markdown ![](https://i.imgur.com/9dgw0h0.png)\n",
        "\n",
        "# Copy taxi-*.csv files from github if they are missing from the runtime.\n",
        "!wget --quiet -nc https://github.com/osipov/training-data-analyst/raw/master/bootcamps/serverless_ml/taxi-11k-datasets.zip  \n",
        "!unzip -q -n taxi-11k-datasets.zip "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "96WuBsX_obIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 1. Refactor the input </h2>\n",
        "\n",
        "The method `read_dataset` was implemented in an earlier lab, but now is the time to make it more general and performant. Instead of using Pandas data frames as the data source, the following implementation relies on the TensorFlow Dataset API."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "C_qGSLyYobIr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
        "LABEL_COLUMN = 'fare_amount'\n",
        "DEFAULTS = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
        "\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "  def _input_fn():\n",
        "    def decode_csv(value_column):\n",
        "      columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
        "      features = dict(zip(CSV_COLUMNS, columns))\n",
        "      label = features.pop(LABEL_COLUMN)\n",
        "      return features, label\n",
        "\n",
        "    # Create list of files that match pattern\n",
        "    file_list = tf.gfile.Glob(filename)\n",
        "\n",
        "    # Create dataset from file list\n",
        "    dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        num_epochs = None # indefinitely\n",
        "        dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "    else:\n",
        "        num_epochs = 1 # end-of-input after this\n",
        "\n",
        "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "    return dataset.make_one_shot_iterator().get_next()\n",
        "  return _input_fn\n",
        "    \n",
        "\n",
        "def get_train():\n",
        "  return read_dataset('./taxi-train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "def get_valid():\n",
        "  return read_dataset('./taxi-valid.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
        "\n",
        "def get_test():\n",
        "  return read_dataset('./taxi-test.csv', mode = tf.estimator.ModeKeys.EVAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kb7Ea_QDobIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> 2. Refactor the way features are created. </h2>\n",
        "\n",
        "For now, pass these through (same as previous lab).  However, refactoring this way will enable us to break the one-to-one relationship between inputs and features."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UQV5jOfJobIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_COLUMNS = [\n",
        "    tf.feature_column.numeric_column('pickuplon'),\n",
        "    tf.feature_column.numeric_column('pickuplat'),\n",
        "    tf.feature_column.numeric_column('dropofflat'),\n",
        "    tf.feature_column.numeric_column('dropofflon'),\n",
        "    tf.feature_column.numeric_column('passengers'),\n",
        "]\n",
        "\n",
        "def add_more_features(feats):\n",
        "  # Nothing to add (yet!)\n",
        "  return feats\n",
        "\n",
        "feature_cols = add_more_features(INPUT_COLUMNS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZinYrLwTobIz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Create and train the model </h2>\n",
        "\n",
        "Note that we train for num_steps * batch_size examples."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9q3hkjQGobI0",
        "colab_type": "code",
        "outputId": "9414b614-2bf7-4549-d3cc-c03a8333980c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "OUTDIR = 'taxi_trained'\n",
        "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
        "\n",
        "model = tf.estimator.LinearRegressor(\n",
        "      feature_columns = feature_cols, model_dir = OUTDIR)\n",
        "\n",
        "model.train(input_fn = get_train(), steps = 100);  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5c749aba10>, '_model_dir': 'taxi_trained', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 124927.875, step = 1\n",
            "INFO:tensorflow:Saving checkpoints for 100 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 47493.484.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NSK4tmzgobI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h3> Evaluate model </h3>\n",
        "\n",
        "As before, evaluate on the validation data.  We'll do the third refactoring (to move the evaluation into the training loop) in the next lab."
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YOs9EXZpobI3",
        "colab_type": "code",
        "outputId": "6fb8015f-3824-4fa7-a6de-3a7dccf05e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "def print_rmse(model, name, input_fn):\n",
        "  metrics = model.evaluate(input_fn = input_fn, steps = 1)\n",
        "  print 'RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss']))\n",
        "  \n",
        "print_rmse(model, 'validation', get_valid())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-03-03T22:33:47Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-100\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [1/1]\n",
            "INFO:tensorflow:Finished evaluation at 2019-03-03-22:33:48\n",
            "INFO:tensorflow:Saving dict for global step 100: average_loss = 127.47322, global_step = 100, label/mean = 12.282267, loss = 65266.29, prediction/mean = 11.297807\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: taxi_trained/model.ckpt-100\n",
            "RMSE on validation dataset = 11.2904043198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rCLqjgO1obI6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Counter Factual .AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}